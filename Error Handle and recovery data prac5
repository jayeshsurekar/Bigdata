sudo apt update
sudo apt install curl -y
sudo apt install gnupg -y
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80\
--recv-keys 2EE0EA64E40A89B84B2DF73499E82A75642AC823

echo"https://repo.scala-sbt.org/scalasbt/debian all main" | \
sudo tee /etc/apt/source.list.d/sbt.list


echo"deb https://repo.scala-sbt.org/scalasbt/debian /" | \
sudo tee /etc/apt/source.list.d/sbt_old.list

curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2
DF73499E82A75642AC823" | sudo apt-key add

sudo apt update
sudo apt install sbt -y

Verify sbt installation
sbt

Install Scala
wget https://downloads.lightbend.com/scala/2.12.18/scala-2.12.18.deb
sudo dpkg -i scala-2.12.18.deb


# enter sbt in the terminal to verify installation
# create the scala program for exception handling
$ nano ExceptionHandlingTest.scala

import org.apache.spark.sql.SparkSession
object ExceptionHandlingTest {
def main(args: Array[String]): Unit = {
val spark = SparkSession
.builder
.appName("ExceptionHandlingTest")
.master("local[*]")// Optional: helpful for local debugging
.getOrCreate()

val rdd = spark.sparkContext.paralleize(0 until spark.sparkContext.defaultParallelism)
rdd.foreach{ i=>
try{
if(math.random > 0.75){
throw new Exception("Testing exception handling")
} else{
printIn(s"Task $i completed successfully")
}
} catch {
case e: Exception =>
printIn(s"Exception in task $i: $(e.getMessage)")
}
}
spark.stop()
}}

Start spark Master and Worker
$SPARK_HOME/$bin/start-master.sh
$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077

Build the Scala project
ls
mkdir ~/ExceptionHandlingProject
cd ~/ExceptionHandlingProject
mkdir -p src/main/scala
mv ~/ExceptionHandlingTest.scala src/main/scala/
nano build.sbt

scalaVersion:= "2.12.18"

libraryDependencies ++=Seq("org.apache.spark"%%"spark-core"%"3.4.1","org.apache.spark"%% "spark-sql" % "3.4.1")

Submint your program to spark
spark~submit\
--class ExceptionHandlingTest\
--master local[*]\
target/scala-2.12/exceptionhandlingproject_2.12-0.1.0-SNAPSHOT.jar
